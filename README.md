# 3D Scene Reconstruction Application

Transform your photos and videos into immersive, navigable 3D environments.

## ğŸ¯ Project Vision

This application processes user-provided images and videos to create interactive 3D scenes that can be explored as if you were physically present in the original location. Whether it's a panoramic video, multiple photos of a room, or a walkthrough recording, our system reconstructs the space in 3D for free navigation.

## ğŸš€ Key Features

- **Multi-source Input**: Process single images, image sequences, panoramic videos, or standard videos
- **3D Scene Generation**: Create navigable 3D meshes and point clouds from 2D inputs
- **Cross-platform Deployment**: Run as a web application or standalone Android app
- **Real-time Navigation**: Walk through, rotate, and zoom within reconstructed scenes
- **View Interpolation**: Generate novel viewpoints not present in original captures

## ğŸ“‹ Technical Overview

### Core Processing Pipeline

```
Input (Images/Video) â†’ Feature Detection â†’ Camera Pose Estimation â†’ 
Dense Reconstruction â†’ Mesh Generation â†’ Texture Mapping â†’ 3D Scene
```

### Technology Stack

#### Backend Processing (Python)
- **OpenCV**: Image/video processing and feature detection
- **COLMAP** or **OpenMVG**: Structure from Motion (SfM) pipeline
- **Open3D**: Point cloud processing and mesh generation
- **NumPy/SciPy**: Mathematical computations
- **Pillow**: Image manipulation
- **FFmpeg-python**: Video frame extraction

#### Modern ML Approaches (Optional)
- **NeRF** (Neural Radiance Fields): Novel view synthesis
- **Gaussian Splatting**: Real-time rendering of 3D scenes
- **MiDaS/DPT**: Monocular depth estimation

#### Web Application
- **FastAPI** or **Flask**: REST API server
- **Three.js**: WebGL-based 3D visualization
- **WebAssembly**: High-performance client-side processing

#### Android Deployment
- **Kivy** or **BeeWare**: Python-to-APK packaging
- **Buildozer**: Android build automation

#### Infrastructure
- **Docker**: Containerized deployment
- **Redis**: Task queue for async processing
- **MinIO**: Object storage for 3D assets

## ğŸ—ï¸ Project Structure

```
3d-scene-reconstruction/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ feature_extraction.py    # SIFT/SURF/ORB detection
â”‚   â”‚   â”œâ”€â”€ camera_calibration.py    # Camera pose estimation
â”‚   â”‚   â”œâ”€â”€ reconstruction.py        # SfM/MVS pipeline
â”‚   â”‚   â”œâ”€â”€ mesh_generation.py       # Point cloud to mesh
â”‚   â”‚   â””â”€â”€ texture_mapping.py       # UV mapping and texturing
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ video_processor.py       # Frame extraction, stabilization
â”‚   â”‚   â”œâ”€â”€ image_processor.py       # Enhancement, normalization
â”‚   â”‚   â””â”€â”€ panorama_processor.py    # Spherical projection handling
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ depth_estimation.py      # Monocular depth networks
â”‚   â”‚   â”œâ”€â”€ nerf_model.py           # NeRF implementation (optional)
â”‚   â”‚   â””â”€â”€ gaussian_splatting.py    # 3DGS implementation (optional)
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ server.py                # FastAPI application
â”‚   â”‚   â”œâ”€â”€ routes.py                # API endpoints
â”‚   â”‚   â””â”€â”€ websocket.py             # Real-time progress updates
â”‚   â”‚
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ scene_exporter.py        # GLTF/OBJ export
â”‚       â””â”€â”€ renderer.py              # Preview generation
â”‚
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js                       # Three.js viewer
â”‚   â””â”€â”€ styles.css
â”‚
â”œâ”€â”€ android/
â”‚   â”œâ”€â”€ buildozer.spec              # Android build config
â”‚   â””â”€â”€ main.py                     # Kivy application entry
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_feature_extraction.py
â”‚   â”‚   â”œâ”€â”€ test_reconstruction.py
â”‚   â”‚   â””â”€â”€ test_mesh_generation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ test_pipeline.py
â”‚   â”‚
â”‚   â””â”€â”€ fixtures/
â”‚       â”œâ”€â”€ sample_images/
â”‚       â””â”€â”€ sample_videos/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.processor        # Processing service
â”‚   â”œâ”€â”€ Dockerfile.web             # Web server
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_colmap.sh            # Install COLMAP binaries
â”‚   â”œâ”€â”€ process_batch.py           # Batch processing utility
â”‚   â””â”€â”€ benchmark.py               # Performance testing
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ CLAUDE.md
â””â”€â”€ README.md
```

## ğŸ”§ Installation

### Using Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/yourusername/3d-scene-reconstruction.git
cd 3d-scene-reconstruction

# Build containers
docker-compose build

# Start services
docker-compose up
```

### Local Development

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install COLMAP (Ubuntu/Debian)
sudo apt-get install colmap

# Install COLMAP (macOS)
brew install colmap

# Run setup script
bash scripts/setup_colmap.sh
```

## ğŸ® Usage Examples

### Basic Pipeline

```python
from src.core import SceneReconstructor
from src.preprocessors import VideoProcessor

# Process a panoramic video
video_processor = VideoProcessor()
frames = video_processor.extract_frames("panorama.mp4", fps=2)

# Reconstruct 3D scene
reconstructor = SceneReconstructor()
scene = reconstructor.process(frames, mode="panoramic")

# Export for web viewing
scene.export("output/scene.gltf")
```

### Web API

```bash
# Upload and process images
curl -X POST http://localhost:8000/api/reconstruct \
  -F "files=@room_photo1.jpg" \
  -F "files=@room_photo2.jpg" \
  -F "files=@room_photo3.jpg"

# Check processing status
curl http://localhost:8000/api/status/{job_id}

# Download 3D scene
curl http://localhost:8000/api/download/{job_id} -o scene.gltf
```

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/unit -v

# Run integration tests
pytest tests/integration -v

# Run with coverage
pytest --cov=src tests/

# Run specific test file
pytest tests/unit/test_reconstruction.py::test_camera_pose_estimation
```

## ğŸ“Š Processing Pipeline Details

### 1. Feature Detection & Matching
- Extract keypoints using SIFT/SURF/ORB
- Match features across multiple views
- Filter matches using RANSAC

### 2. Structure from Motion (SfM)
- Estimate camera poses
- Triangulate 3D points
- Bundle adjustment optimization

### 3. Dense Reconstruction
- Multi-View Stereo (MVS) for dense point clouds
- Depth map fusion
- Point cloud filtering and cleaning

### 4. Surface Reconstruction
- Poisson surface reconstruction
- Mesh simplification
- Texture atlas generation

### 5. Rendering & Export
- GLTF/GLB export for web
- OBJ/MTL for compatibility
- Point cloud formats (PLY, PCD)

## ğŸ¯ Roadmap

### Phase 1: Core Pipeline (Current)
- [x] Project structure setup
- [ ] Basic SfM implementation
- [ ] Point cloud generation
- [ ] Simple mesh reconstruction

### Phase 2: Enhancement
- [ ] Improved texture mapping
- [ ] Panorama-specific optimizations
- [ ] Real-time preview during processing
- [ ] GPU acceleration (CUDA/OpenCL)

### Phase 3: Advanced Features
- [ ] NeRF integration for view synthesis
- [ ] Gaussian Splatting for real-time rendering
- [ ] AI-powered scene completion
- [ ] Semantic segmentation of scenes

### Phase 4: Deployment
- [ ] Web application optimization
- [ ] Android APK packaging
- [ ] Cloud deployment (GCP/Kubernetes)
- [ ] Processing queue with RabbitMQ

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“š References & Resources

- [COLMAP Documentation](https://colmap.github.io/)
- [Open3D Documentation](http://www.open3d.org/docs/)
- [NeRF: Neural Radiance Fields](https://www.matthewtancik.com/nerf)
- [3D Gaussian Splatting](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)
- [Structure from Motion Tutorial](https://www.cs.cornell.edu/~snavely/bundler/bundler-v0.4-manual.html)

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- COLMAP team for the excellent SfM framework
- Open3D community for 3D processing tools
- Three.js for WebGL visualization
